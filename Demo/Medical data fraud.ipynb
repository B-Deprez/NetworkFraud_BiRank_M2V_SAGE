{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af930dfc",
   "metadata": {},
   "source": [
    "# Demo Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33aa2b",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e4fdf",
   "metadata": {},
   "source": [
    "In this notebook, we give a demo of how to use social network analysis for insurance fraud detection. \n",
    "\n",
    "The data for this analysis is taken from Kaggle https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis in order to ensure repreducibility. \n",
    "\n",
    "We are going to start with loading the necessary packages and own implementation of the methods. Afterwards, we inspect the data. With these insights, we construct the network on which we will do further analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d49463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HelperFunctions import load_network\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import excecute\n",
    "from HelperFunctions import to_bipartite, feature_engineering\n",
    "from BiRank import *\n",
    "from metapath2vec import *\n",
    "from stellargraph import StellarGraph\n",
    "import networkx as nx\n",
    "import Metrics\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d270bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73365426",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf788b",
   "metadata": {},
   "source": [
    "There are two sorts of patients, both of which we will use. The Inpatient data consists of patients that were admitted to the hospital, while the Outpatient data covers patients that visited the hospital, but were not admitted in it. Addtional fraud labels are present in a seperate data set. There are only fraud labels for the health care providers. Hence, we are  going to look for suspicious providers, not claims. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1af6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incoming = pd.read_csv(\"data/archive (1)/Train_Inpatientdata-1542865627584.csv\")\n",
    "Outgoing = pd.read_csv(\"data/archive (1)/Train_Outpatientdata-1542865627584.csv\")\n",
    "labels = pd.read_csv(\"data/archive (1)/Train-1542865627584.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f181eda",
   "metadata": {},
   "source": [
    "Below, we do a very quick inspection of the different tables, in order to have a sense of the naming of the different features, and the format in which the fraud labels are provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incoming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outgoing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07324327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb5520",
   "metadata": {},
   "source": [
    "All data on patients is put together in one data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90354f8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AllClaims = [Incoming, Outgoing]\n",
    "AllClaims = pd.concat(AllClaims)\n",
    "AllClaims.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ff50a",
   "metadata": {},
   "source": [
    "## Setting up the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91b025",
   "metadata": {},
   "source": [
    "In order to be able to work with the network via the stellargraph package, the network data, i.e., the nodes and edges, need to be in a specific format. We need data frames for the nodes, where the nodes are the index of the data frame, and the name of this index is \"ID\". This is done below. \n",
    "\n",
    "Note that we are going to extract the data from the tables per node type. This makes it possible to take the heterogeneity of the different nodes into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bene_nodes = AllClaims[[\"BeneID\"]].rename(columns={\"BeneID\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "claim_nodes = AllClaims[[\"ClaimID\"]].rename(columns={\"ClaimID\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "provider_nodes = AllClaims[[\"Provider\"]].rename(columns={\"Provider\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "physician_nodes = AllClaims[[\"AttendingPhysician\"]].rename(columns={\"AttendingPhysician\":\"ID\"}).drop_duplicates().set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d5bb4",
   "metadata": {},
   "source": [
    "For the edges of the network, we will just take them as presented in the original data, i.e., we connect the claims with the other nodes as given in the entry. Here, the columns need to be named \"source\" and \"target\" in order for it to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af82c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bene_claim = AllClaims[[\"BeneID\", \"ClaimID\"]].rename(columns={\"BeneID\":\"source\", \"ClaimID\":\"target\"})\n",
    "provider_claim = AllClaims[[\"Provider\", \"ClaimID\"]].rename(columns={\"Provider\":\"source\", \"ClaimID\":\"target\"})\n",
    "phys_claim = AllClaims[[\"AttendingPhysician\", \"ClaimID\"]].rename(columns={\"AttendingPhysician\":\"source\", \"ClaimID\":\"target\"})\n",
    "edges = pd.concat([bene_claim, provider_claim, phys_claim]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbeb1ab",
   "metadata": {},
   "source": [
    "With everything set up, we can easily construct the network with stellargraph. We will transform it also to networkx since it will make it easier to do some of the calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "HG = StellarGraph({\"claim\": claim_nodes, \"beneficiary\": bene_nodes, \"provider\": provider_nodes, \"physician\": physician_nodes}, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "HG_nx = HG.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428ec78",
   "metadata": {},
   "source": [
    "Below, we are going to inspect the structure of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4778c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HG.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453884",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = HG.node_degrees()\n",
    "bene_deg = pd.Series([degrees[x] for x in list(bene_nodes.index)])\n",
    "claim_deg = pd.Series([degrees[x] for x in list(claim_nodes.index)])\n",
    "provider_deg = pd.Series([degrees[x] for x in list(provider_nodes.index)])\n",
    "physician_deg = pd.Series([degrees[x] for x in list(physician_nodes.index)])\n",
    "perc = [.25,.50,.75,.85,.95, .99] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_deg.describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bene_deg.describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_deg.describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d94bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "physician_deg.describe(percentiles = perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d97d33",
   "metadata": {},
   "source": [
    "In order to be able to incorportate the fraud labels, they need to be put into 0-1. Also, the providers need to be the index of the table, and the column with fraud labels for the algorithms needs to be called \"Fraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.set_index(\"Provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = [0 if x == \"No\" else 1 for x in labels[\"PotentialFraud\"]]\n",
    "labels[\"Fraud\"] = num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cac40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f66b0",
   "metadata": {},
   "source": [
    "Since everything is now numerical, we can quickly see the relative number of fraudulent providers. Below, we see that is is around 9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[\"Fraud\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d9621",
   "metadata": {},
   "source": [
    "With everything nicely set up, we can move to the calculation of the different network embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf639f",
   "metadata": {},
   "source": [
    "## Network Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f3714",
   "metadata": {},
   "source": [
    "### Metapath2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c585e",
   "metadata": {},
   "source": [
    "The first embedding algorithm we are going to consider is the metapath2vec method. This method walks around in the network in order to form random paths, which can be converted into an embedding using NLP methods. Here, we restrict the possible paths to a couple of metapath.\n",
    "\n",
    "Below, one can see that each path needs to start and stop in a provider node, and that we do not allow long detours in order to get there. \n",
    "\n",
    "These metapaths are fed into the method, which is already implementede in its dedicated python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50561e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metapaths = [\n",
    "            [\"provider\", \"claim\", \"provider\"],\n",
    "            [\"provider\", \"claim\",\"physician\", \"claim\", \"provider\"],\n",
    "            [\"provider\", \"claim\",\"beneficiary\", \"claim\", \"provider\"]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids, node_embeddings, node_targets = Metapath2vec(HG,\n",
    "                                                       metapaths, \n",
    "                                                       dimensions = 64,\n",
    "                                                       num_walks = 2,\n",
    "                                                       walk_length = 7,\n",
    "                                                       context_window_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e78425",
   "metadata": {},
   "source": [
    "With the embeddings constructed, we need to link the right embedding with the corresponding provider and hence with the corresponding fraud label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aa357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HG.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(node_targets)) if node_targets[i]==\"provider\" ]\n",
    "providers_sorted = [node_ids[i] for i in range(len(node_targets)) if node_targets[i]==\"provider\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ead0e4",
   "metadata": {},
   "source": [
    "These embeddings are used to construct a fraud detection model. For this, we are going to use a gradient boosting classifier as implemented in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b12e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = node_embeddings[indices]\n",
    "X = pd.DataFrame(X).sort_index()\n",
    "y = labels.sort_index().loc[providers_sorted][\"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(round(0.6 * len(y), 0)); train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:train_size,:]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_test = X.iloc[train_size:,:]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8852a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                                 max_depth=2).fit(X_train, y_train)\n",
    "\n",
    "y_pred_meta = embedding_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef7e50",
   "metadata": {},
   "source": [
    "The quality of the model is analysed using the AUC, the average precision and the lift curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6253f80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr_meta, tpr_meta, thresholds = metrics.roc_curve(y_test, y_pred_meta)\n",
    "plt.plot(fpr_meta, tpr_meta)\n",
    "plt.plot([0, 1], [0, 1], color=\"grey\", alpha=0.5)\n",
    "plt.title(\"AUC-ROC: \" + str(np.round(metrics.auc(fpr_meta, tpr_meta), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_meta, recall_meta, thresholds = metrics.precision_recall_curve(y_test,y_pred_meta)\n",
    "AP_meta = np.round(metrics.average_precision_score(y_test, y_pred_meta),3)\n",
    "plt.plot(recall_meta,precision_meta, label =str(AP_meta), alpha =0.7)\n",
    "plt.title(\"AUC-PRC\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7871ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(start = 0.05, stop = 1, step = 0.001)\n",
    "lft_meta = Metrics.lift_curve_values(y_test, y_pred_meta, steps)\n",
    "plt.plot(steps, lft_meta, alpha = 0.7)\n",
    "\n",
    "plt.title(\"Lift Curve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c78d8",
   "metadata": {},
   "source": [
    "### BiRank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08fa50",
   "metadata": {},
   "source": [
    "The next method is BiRank. Here, we need to reconstruct the network into a bipartite one. We do this by connecting the providers (group 1) to the corresponding physicians and beneficiaries (together group 2). We say that an edge is present between a provider and a physician/beneficiary, if the are part of the same claim. \n",
    "\n",
    "We need to do some simple matrix multiplications in order to obtain the correct adjacency matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34816b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "HG_claims = HG.nodes(\"claim\")\n",
    "HG_physicians = HG.nodes(\"physician\")\n",
    "HG_providers = providers_sorted#HG.nodes(\"provider\")\n",
    "HG_beneficiaries = HG.nodes(\"beneficiary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca50f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_C_Ph = HG.to_adjacency_matrix(\n",
    "    list(HG_claims) + list(HG_physicians)\n",
    ")[ :len(HG_claims), len(HG_claims): ]\n",
    "\n",
    "adj_C_B = HG.to_adjacency_matrix(\n",
    "    list(HG_claims) + list(HG_beneficiaries)\n",
    ")[ :len(HG_claims), len(HG_claims): ]\n",
    "\n",
    "adj_C_Pr = HG.to_adjacency_matrix(\n",
    "    list(HG_claims) + list(HG_providers)\n",
    ")[ :len(HG_claims), len(HG_claims): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d53244",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_Pr_B = adj_C_Pr.transpose() @ adj_C_B\n",
    "adj_Pr_Ph = adj_C_Pr.transpose() @ adj_C_Pr\n",
    "\n",
    "adjmat_bipartite = sp.sparse.hstack((adj_Pr_B, adj_Pr_Ph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1aa24",
   "metadata": {},
   "source": [
    "This new bipartite adjacency matrix will now be used for the calculation of the fraud scores for the medical providers. \n",
    "\n",
    "The BiRank method will be applied twice in order to take the temporal aspect into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = {\"FraudInd\": labels[\"Fraud\"][HG_providers].values}\n",
    "fraudMat = pd.DataFrame(fraud)\n",
    "\n",
    "provider_nodes = pd.DataFrame({\"ID\": HG_providers}).set_index(\"ID\")\n",
    "\n",
    "HG_parties = np.concatenate((HG_beneficiaries, HG_physicians))\n",
    "party_nodes = pd.DataFrame({\"ID\": HG_parties}).set_index(\"ID\")\n",
    "\n",
    "ADJ = adjmat_bipartite.transpose().tocsr()\n",
    "\n",
    "Claims_res, Parties_res, aMat, iterations, convergence = BiRank(ADJ, provider_nodes, party_nodes, fraudMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf5e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ADJ = adjmat_bipartite.tocsr()\n",
    "number_providers = ADJ.shape[0]\n",
    "train_set_size = int(np.round(0.6*number_providers))\n",
    "test_set_size = number_providers-train_set_size\n",
    "ADJ_train= ADJ[:train_set_size,:]\n",
    "fraudMat_train = fraudMat.iloc[:train_set_size]\n",
    "provider_train = provider_nodes[:train_set_size]\n",
    "\n",
    "ADJ = ADJ_train.transpose().tocsr()\n",
    "Claims_res, Parties_res, aMat, iterations, convergence = BiRank(ADJ, provider_train, party_nodes, fraudMat_train)\n",
    "\n",
    "fraud_train_res = {\"FraudInd\": Claims_res.sort_values(\"ID\")[\"ScaledScore\"].values}\n",
    "test_set_fraud = {\"FraudInd\": [0]*test_set_size}\n",
    "fraudMat_train_res = pd.DataFrame(fraud_train_res)\n",
    "fraudMat_test_set = pd.DataFrame(test_set_fraud)\n",
    "fraudMat_test = fraudMat_train_res.append(fraudMat_test_set)\n",
    "\n",
    "ADJ = adjmat_bipartite.transpose().tocsr()\n",
    "Claims_res, Parties_res, aMat, iterations, convergence = BiRank(ADJ, provider_nodes, party_nodes, fraudMat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80215e2e",
   "metadata": {},
   "source": [
    "As before, we look at the models performance using the AUC, AP and lift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = fraudMat.iloc[train_set_size:][\"FraudInd\"]\n",
    "y_pred_bi = Claims_res.sort_values(\"ID\")[train_set_size:].ScaledScore\n",
    "fpr_bi, tpr_bi, thresholds = metrics.roc_curve(y_test,y_pred_bi)\n",
    "\n",
    "plt.plot(fpr_bi,tpr_bi)\n",
    "\n",
    "plt.plot([0,1], [0,1], color = \"grey\", alpha = 0.5)\n",
    "plt.title(\"AUC-ROC: \"+str(np.round(metrics.auc(fpr_bi, tpr_bi),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_bi, recall_bi, thresholds = metrics.precision_recall_curve(y_test,y_pred_bi)\n",
    "AP_bi = np.round(metrics.average_precision_score(y_test, y_pred_bi),3)\n",
    "plt.plot(recall_bi,precision_bi, label =str(AP_bi), alpha =0.7)\n",
    "plt.title(\"AUC-PRC\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(start = 0.05, stop = 1, step = 0.001)\n",
    "lft_bi = Metrics.lift_curve_values(y_test, y_pred_bi, steps)\n",
    "plt.plot(steps, lft_bi, alpha = 0.7)\n",
    "\n",
    "plt.title(\"Lift Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ed141",
   "metadata": {},
   "source": [
    "### Traditional Network Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9019b",
   "metadata": {},
   "source": [
    "The final set of features we are going to look at, are the more traditional ones. It consists of the degree and the betweenness centrality. These are also much easier to interprete than the other ones. \n",
    "\n",
    "We have run the code (now in text) before, and saved the results in an excel file. This saves us time when rerunning the notebook. \n",
    "\n",
    "As always, the model performance is analysed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from HelperFunctions import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_geo_G = geodesic(HG_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_cen = nx.degree_centrality(HG_nx)\n",
    "df_degcen = pd.DataFrame({'provider': [provider for provider in provider_nodes.index],\n",
    "                          'degree': [deg_cen[provider] for provider in provider_nodes.index] })\n",
    "df_degcen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4f96f",
   "metadata": {},
   "source": [
    "from HelperFunctions import load_network, feature_engineering\n",
    "import networkx as nx\n",
    "import networkit\n",
    "import pandas as pd\n",
    "\n",
    "nx_graph_1 = HG.to_networkx()\n",
    "nx_graph_2 = nx.Graph(nx_graph_1)\n",
    "G_nit = networkit.nxadapter.nx2nk(nx_graph_2)\n",
    "\n",
    "zipped_nodes = zip(nx_graph_2.nodes(), range(nx_graph_2.number_of_nodes()))\n",
    "node_keys = pd.DataFrame(zipped_nodes)\n",
    "\n",
    "cl_cen = networkit.centrality.ApproxCloseness(G_nit, 10000).run().ranking()\n",
    "cl_cen_df = pd.DataFrame(cl_cen)\n",
    "\n",
    "cl_cen_nodes = node_keys.merge(cl_cen_df, left_on = 1, right_on = 0)\n",
    "\n",
    "cl_cen_nodes = cl_cen_nodes[[\"0_x\", \"1_y\"]]\n",
    "cl_cen_nodes.columns= [\"node_id\", \"Closeness Centrality\"]\n",
    "\n",
    "btw_cen = networkit.centrality.EstimateBetweenness(G_nit, 10000).run().ranking()\n",
    "\n",
    "btw_cen_df = pd.DataFrame(btw_cen)\n",
    "btw_cen_nodes = node_keys.merge(btw_cen_df, left_on = 1, right_on = 0)\n",
    "btw_cen_nodes = btw_cen_nodes[[\"0_x\", \"1_y\"]]\n",
    "btw_cen_nodes.columns= [\"node_id\", \"Betweenness Centrality\"]\n",
    "\n",
    "centralities = cl_cen_nodes.merge(btw_cen_nodes, on = \"node_id\")\n",
    "centralities.to_csv(\"Centralities_medical.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c0028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Centralities_medical = pd.read_csv(\"Centralities_medical.csv\").set_index(\"node_id\")\n",
    "Centralities_medical = Centralities_medical.loc[labels.index]\n",
    "\n",
    "deg_cen = nx.degree_centrality(HG_nx)\n",
    "df_degcen = pd.DataFrame({'provider': [provider for provider in provider_nodes.index],\n",
    "                          'degree': [deg_cen[provider] for provider in provider_nodes.index] }).set_index(\"provider\").loc[labels.index]\n",
    "\n",
    "X = pd.merge(Centralities_medical, df_degcen, left_index = True, right_index = True).dropna().sort_index()\n",
    "\n",
    "train_size = int(round(0.6 * len(labels), 0))\n",
    "\n",
    "y = labels.sort_index()[\"Fraud\"][X.index]\n",
    "\n",
    "X_train = X.iloc[:train_size,:]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_test = X.iloc[train_size:,:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "embedding_model = GradientBoostingClassifier(n_estimators=500,\n",
    "                                                 subsample=0.8,\n",
    "                                                 max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "y_pred_cen = embedding_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd133b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_cen, tpr_cen, thresholds = metrics.roc_curve(y_test, y_pred_cen)\n",
    "plt.plot(fpr_cen, tpr_cen)\n",
    "plt.plot([0, 1], [0, 1], color=\"grey\", alpha=0.5)\n",
    "plt.title(\"AUC-ROC: \" + str(np.round(metrics.auc(fpr_cen, tpr_cen), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_cen, recall_cen, thresholds = metrics.precision_recall_curve(y_test,y_pred_cen)\n",
    "AP_cen = np.round(metrics.average_precision_score(y_test, y_pred_cen),3)\n",
    "plt.plot(recall_cen,precision_cen, label =str(AP_cen), alpha =0.7)\n",
    "plt.title(\"AUC-PRC\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3861342",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps = np.arange(start = 0.05, stop = 1, step = 0.001)\n",
    "lft_cen = Metrics.lift_curve_values(y_test, y_pred_cen, steps)\n",
    "plt.plot(steps, lft_cen, alpha = 0.7)\n",
    "\n",
    "plt.title(\"Lift Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a621f",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stellargraph.layer import HinSAGE    \n",
    "from stellargraph.mapper import HinSAGENodeGenerator, NodeSequence\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers, optimizers, Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8739b8",
   "metadata": {},
   "source": [
    "We are going to re-initialise the data set, to be sure that no data problems will arise down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456acfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incoming = pd.read_csv(\"data/archive (1)/Train_Inpatientdata-1542865627584.csv\")\n",
    "Outgoing = pd.read_csv(\"data/archive (1)/Train_Outpatientdata-1542865627584.csv\")\n",
    "ben_data = pd.read_csv(\"data/archive (1)/Train_Beneficiarydata-1542865627584.csv\")\n",
    "labels = pd.read_csv(\"data/archive (1)/Train-1542865627584.csv\")\n",
    "\n",
    "AllClaims = [Incoming, Outgoing]\n",
    "AllClaims = pd.concat(AllClaims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edf93c",
   "metadata": {},
   "source": [
    "Using the data, we can easily construct the network, consisting of nodes and edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bene_nodes = AllClaims[[\"BeneID\"]].rename(columns={\"BeneID\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "claim_nodes = AllClaims[[\"ClaimID\"]].rename(columns={\"ClaimID\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "provider_nodes = AllClaims[[\"Provider\"]].rename(columns={\"Provider\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "physician_nodes = AllClaims[[\"AttendingPhysician\"]].rename(columns={\"AttendingPhysician\":\"ID\"}).drop_duplicates().set_index(\"ID\")\n",
    "\n",
    "nodes = {\"beneficiary\": bene_nodes, \"claim\": claim_nodes, \"provider\": provider_nodes, \"physician\": physician_nodes}\n",
    "\n",
    "edges = [\n",
    "    zip(AllClaims.BeneID, AllClaims.ClaimID),\n",
    "    zip(AllClaims.Provider, AllClaims.ClaimID),\n",
    "    zip(AllClaims.AttendingPhysician, AllClaims.ClaimID)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f2783",
   "metadata": {},
   "source": [
    "For the previous methods, we did not have additional feature information for the health care providers. Now, with GraphSAGE, we will incorporate the features of the other kinds of nodes in the network. \n",
    "\n",
    "Especially the patients have a lot of data being kept with their dosier. In addition, we incorporate two features for the claim data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e2127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beneficiary_data = ben_data.set_index(\"BeneID\")\n",
    "beneficiary_data[\"Deceased\"] = [0 if x else 1 for x in beneficiary_data[\"DOD\"].isna()]\n",
    "beneficiary_data[\"RenalDiseaseIndicator_01\"] = [0 if x == 0 else 1 for x in ben_data[\"RenalDiseaseIndicator\"]]\n",
    "beneficiary_data[\"Gender_01\"] = beneficiary_data[\"Gender\"]-1\n",
    "beneficiary_data[\"ChronicCond_Alzheimer_01\"] = beneficiary_data[\"ChronicCond_Alzheimer\"]-1\n",
    "beneficiary_data[\"ChronicCond_Heartfailure_01\"] = beneficiary_data[\"ChronicCond_Heartfailure\"]-1\n",
    "beneficiary_data[\"ChronicCond_KidneyDisease_01\"] = beneficiary_data[\"ChronicCond_KidneyDisease\"]-1\n",
    "beneficiary_data[\"ChronicCond_Cancer_01\"] = beneficiary_data[\"ChronicCond_Cancer\"]-1\n",
    "beneficiary_data[\"ChronicCond_ObstrPulmonary_01\"] = beneficiary_data[\"ChronicCond_ObstrPulmonary\"]-1\n",
    "beneficiary_data[\"ChronicCond_Depression_01\"] = beneficiary_data[\"ChronicCond_Depression\"]-1\n",
    "beneficiary_data[\"ChronicCond_Diabetes_01\"] = beneficiary_data[\"ChronicCond_Diabetes\"]-1\n",
    "beneficiary_data[\"ChronicCond_IschemicHeart_01\"] = beneficiary_data[\"ChronicCond_IschemicHeart\"]-1\n",
    "beneficiary_data[\"ChronicCond_Osteoporasis_01\"] = beneficiary_data[\"ChronicCond_Osteoporasis\"]-1\n",
    "beneficiary_data[\"ChronicCond_rheumatoidarthritis_01\"] = beneficiary_data[\"ChronicCond_rheumatoidarthritis\"]-1\n",
    "beneficiary_data[\"ChronicCond_stroke_01\"] = beneficiary_data[\"ChronicCond_stroke\"]-1\n",
    "\n",
    "selected_features = [\n",
    "    #\"DOB\",\n",
    "    \"Deceased\",\n",
    "    \"Race\",\n",
    "    \"RenalDiseaseIndicator_01\",\n",
    "    \"State\",\n",
    "    \"Gender_01\",\n",
    "    \"ChronicCond_Alzheimer_01\",\n",
    "    \"ChronicCond_Heartfailure_01\",\n",
    "    \"ChronicCond_KidneyDisease_01\",\n",
    "    \"ChronicCond_Cancer_01\",\n",
    "    \"ChronicCond_ObstrPulmonary_01\",\n",
    "    \"ChronicCond_Depression_01\",\n",
    "    \"ChronicCond_Diabetes_01\",\n",
    "    \"ChronicCond_IschemicHeart_01\",\n",
    "    \"ChronicCond_Osteoporasis_01\",\n",
    "    \"ChronicCond_rheumatoidarthritis_01\",\n",
    "    \"ChronicCond_stroke_01\"\n",
    "]\n",
    "\n",
    "beneficiary_data[\"DOB\"] = pd.to_datetime(beneficiary_data[\"DOB\"])\n",
    "\n",
    "beneficiary_data = beneficiary_data[selected_features]\n",
    "beneficiary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllClaims[\"DurationStay\"] =pd.to_numeric(pd.to_datetime(AllClaims[\"ClaimEndDt\"])-pd.to_datetime(AllClaims[\"ClaimStartDt\"]))/86400000000000\n",
    "\n",
    "claim_data = AllClaims.set_index(\"ClaimID\")\n",
    "\n",
    "selected_features = [\n",
    "    \"DurationStay\",\n",
    "    \"InscClaimAmtReimbursed\"\n",
    "]\n",
    "\n",
    "claim_data[\"ClaimStartDt\"] = pd.to_datetime(claim_data[\"ClaimStartDt\"])\n",
    "\n",
    "claim_data = claim_data[selected_features]\n",
    "claim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8353f40",
   "metadata": {},
   "source": [
    "The GraphSAGE implementation in StellarGraph requires to have features present for all nodes. However, both the providers and physicians do not have additional information. Therefore, we set their features equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dab823",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_data = provider_nodes.copy(deep = False)\n",
    "provider_data[\"Feature\"] = 1 \n",
    "\n",
    "physician_data = physician_nodes.copy(deep = False)\n",
    "physician_data[\"Feature\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0db1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\"beneficiary\": beneficiary_data, \"claim\": claim_data, \"provider\": provider_data, \"physician\": physician_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d29249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConstruction:\n",
    "    \n",
    "    \"\"\"\n",
    "    This class initializes a networkX graph\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    nodes : dict(str, iterable)\n",
    "        A dictionary with keys representing the node type, values representing\n",
    "        an iterable container of nodes (list, dict, set etc.)\n",
    "    edges : 2-tuples (u,v) or 3-tuples (u,v,d)\n",
    "        Each edge given in the container will be added to the graph. \n",
    "    features: dict(str, (str/dict/list/Dataframe)\n",
    "        A dictionary with keys representing node type, values representing the node\n",
    "        data.      \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    g_nx = None\n",
    "    node_features = None\n",
    "    \n",
    "    def __init__(self, nodes, edges, features = None):\n",
    "        self.g_nx = nx.Graph()\n",
    "        self.add_nodes(nodes)\n",
    "        self.add_edges(edges)\n",
    "        \n",
    "        if features is not None:\n",
    "            self.node_features = features\n",
    "    \n",
    "    def add_nodes(self, nodes):\n",
    "        \n",
    "        for key, values in nodes.items():\n",
    "            self.g_nx.add_nodes_from(list(values.index), ntype=key)   \n",
    "            \n",
    "    def add_edges(self, edges):\n",
    "        \n",
    "        for edge in edges:\n",
    "            self.g_nx.add_edges_from(edge)\n",
    "            \n",
    "    def get_stellargraph(self):\n",
    "        return sg.StellarGraph.from_networkx(self.g_nx, node_type_attr=\"ntype\", node_features=self.node_features) #sg.StellarGraph.from_networkx(self.g_nx) \n",
    "    \n",
    "    def get_edgelist(self):\n",
    "        edgelist = []\n",
    "        for edge in nx.generate_edgelist(self.g_nx):\n",
    "            edgelist.append(str(edge).strip('{}'))\n",
    "        el = []\n",
    "        for edge in edgelist:\n",
    "            splitted = edge.split()\n",
    "            numeric = map(float,splitted)\n",
    "            el.append(list(numeric))\n",
    "        return el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphConstruction(nodes, edges, features)\n",
    "S = graph.get_stellargraph()\n",
    "print(S.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[\"Fraud\"] = [0 if x == \"No\" else 1 for x in labels[\"PotentialFraud\"]]\n",
    "labels\n",
    "labels_indexed = labels.set_index(\"Provider\")[[\"Fraud\"]]\n",
    "\n",
    "train_size = int(round(0.5 * len(y), 0))\n",
    "val_test_size = int(round(0.6 * len(y), 0)) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f75fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects, test_subjects = train_test_split(\n",
    "    labels_indexed, train_size=train_size, test_size=None, stratify=labels_indexed\n",
    ")\n",
    "val_subjects, test_subjects = train_test_split(\n",
    "    test_subjects, train_size=val_test_size, test_size=None, stratify=test_subjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "num_samples = [2,32]\n",
    "embedding_node_type = \"provider\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9e86d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator = HinSAGENodeGenerator(S, batch_size, num_samples, head_node_type = embedding_node_type)\n",
    "train_gen = generator.flow(train_subjects.index, train_subjects[\"Fraud\"])\n",
    "val_gen = generator.flow(val_subjects.index, val_subjects[\"Fraud\"])\n",
    "\n",
    "model = HinSAGE(layer_sizes = [64, 64], generator = generator, dropout=0)\n",
    "x_inp, x_out = model.build() \n",
    "prediction = layers.Dense(units=1, activation=\"sigmoid\", dtype='float32')(x_out)\n",
    "model = Model(inputs = x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adam(lr=1e-3),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"AUC\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es_callback = EarlyStopping(monitor=\"val_auc\", patience=5, restore_best_weights=True)\n",
    "\n",
    "import sklearn\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "                                            classes = np.unique(labels[\"Fraud\"]),\n",
    "                                            y = labels[\"Fraud\"].values)\n",
    "\n",
    "class_weight = {0: weights[0], 1: weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen, \n",
    "    epochs = 50, \n",
    "    validation_data = val_gen,\n",
    "    shuffle=False, # this should be False, since shuffling data means shuffling the whole graph\n",
    "    verbose = 2,\n",
    "    callbacks=[es_callback],\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen =generator.flow(test_subjects.index, test_subjects[\"Fraud\"])\n",
    "\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Subject\": test_subjects.index, \n",
    "    \"Label\": test_subjects[\"Fraud\"],\n",
    "    \"Predicted\": test_predictions.squeeze()\n",
    "})\n",
    "\n",
    "y_test = df[\"Label\"]\n",
    "y_pred_sage = df[\"Predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_sage, tpr_sage, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_sage)\n",
    "plt.plot(fpr_sage, tpr_sage)\n",
    "plt.plot([0,1],[0,1], color = \"grey\", alpha =0.5)\n",
    "plt.title(\"AUC-ROC: \" + str(np.round(sklearn.metrics.auc(fpr_sage, tpr_sage), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd553e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_sage, recall_sage, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_pred_sage)\n",
    "AP_sage = np.round(sklearn.metrics.average_precision_score(y_test, y_pred_sage), 3)\n",
    "plt.plot(recall_sage, precision_sage, label = str(AP_sage), alpha =0.7)\n",
    "plt.title(\"AUC-PRC\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(start = 0.05, stop = 1, step = 0.001)\n",
    "lft_sage = Metrics.lift_curve_values(y_test, y_pred_sage, steps)\n",
    "plt.plot(steps, lft_cen, alpha = 0.7)\n",
    "plt.title(\"Lift Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0da012",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de59c40",
   "metadata": {},
   "source": [
    "This final part of the demo puts all results together. In this way, we can use the different performance metric to compare the models with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_cen, tpr_cen, alpha = 0.7, label =  \"Simple Network Features: \" + str(np.round(sklearn.metrics.auc(fpr_cen, tpr_cen), 3)))\n",
    "plt.plot(fpr_bi, tpr_bi, alpha = 0.7, label =  \"BiRank: \" + str(np.round(sklearn.metrics.auc(fpr_bi, tpr_bi), 3)))\n",
    "plt.plot(fpr_meta, tpr_meta, alpha = 0.7, label =  \"Metapath2Vec: \" + str(np.round(sklearn.metrics.auc(fpr_meta, tpr_meta), 3)))\n",
    "plt.plot(fpr_sage, tpr_sage, alpha = 0.7, label =  \"GraphSAGE: \" + str(np.round(sklearn.metrics.auc(fpr_sage, tpr_sage), 3)))\n",
    "plt.plot([0, 1], [0, 1], color=\"grey\", alpha=0.5)\n",
    "plt.title(\"AUC-ROC\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.savefig(\"AUC_health.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall_cen,precision_cen, alpha = 0.7, label =  \"Simple Network Features: \" + str(AP_cen))\n",
    "plt.plot(recall_bi,precision_bi, alpha = 0.7, label =  \"BiRank: \" + str(AP_bi))\n",
    "plt.plot(recall_meta,precision_meta, alpha =0.7, label =  \"Metapath2Vec: \" + str(AP_meta))\n",
    "plt.plot(recall_sage,precision_sage, alpha =0.7, label =  \"GraphSAGE: \" + str(AP_sage))\n",
    "plt.title(\"AUC-PRC\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.savefig(\"AP_health.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ab03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps,lft_cen, alpha = 0.7, label =  \"Simple Network Features\")\n",
    "plt.plot(steps,lft_bi, alpha = 0.7, label =  \"BiRank\")\n",
    "plt.plot(steps,lft_meta, alpha =0.7, label =  \"Metapath2Vec\")\n",
    "plt.plot(steps,lft_sage, alpha =0.7, label =  \"GraphSAGE\")\n",
    "plt.title(\"Lift Curve\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"Lift\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Lift_health.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0354f",
   "metadata": {},
   "source": [
    "From these figures, we can conclude that the simple network features seem to be competitive and sometimes outperform the models based on more complex, and less interpretable, network embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c5a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
